{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGmDnWw5Klo9"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.LabelField()\n",
        "\n",
        "# load data splits\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL)\n",
        "\n",
        "# build dictionary\n",
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "# hyperparameters\n",
        "vocab_size = len(TEXT.vocab)\n",
        "label_size = len(LABEL.vocab)\n",
        "padding_idx = TEXT.vocab.stoi['<pad>']\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "\n",
        "# build iterators\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhQ_58tT79tt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lowKfixYF8kP"
      },
      "source": [
        "problem 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "eaU6L_OKKm2K",
        "outputId": "6a937923-0fc6-4788-b7d2-937bd9277294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (17.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWj9LcZ1K-Lo"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        text = batch.text\n",
        "        predictions = model(text).squeeze(1)\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        preds = torch.argmax(predictions, dim=1)\n",
        "        correct_preds += (preds == batch.label).sum().item()\n",
        "        total_preds += len(batch.label)\n",
        "\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss / len(iterator), accuracy\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text = batch.text\n",
        "            predictions = model(text).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            preds = torch.argmax(predictions, dim=1)\n",
        "            correct_preds += (preds == batch.label).sum().item()\n",
        "            total_preds += len(batch.label)\n",
        "\n",
        "    accuracy = correct_preds / total_preds\n",
        "    return epoch_loss / len(iterator), accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rErW1e1s0sqY"
      },
      "outputs": [],
      "source": [
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, label_size, padding_idx):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.label_size = label_size\n",
        "        self.num_layers = 1\n",
        "        # 1. Embedding Layer\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, padding_idx=padding_idx)\n",
        "\n",
        "        self.rnn = nn.RNN(self.embedding_dim, self.hidden_dim, self.num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(self.hidden_dim, self.hidden_dim+200)\n",
        "        self.fc2= nn.Linear(self.hidden_dim+200, self.label_size)\n",
        "\n",
        "\n",
        "    def zero_state(self, batch_size):\n",
        "        # implement the function, which returns an initial hidden state.\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim)\n",
        "        return h0\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "\n",
        "\n",
        "        batch_size = text.size(0)\n",
        "        h0 = self.zero_state(batch_size)\n",
        "        rnn_out, _ = self.rnn(embedded, h0)\n",
        "\n",
        "        final_hidden_state = rnn_out[:, -1, :]\n",
        "\n",
        "        x = self.fc1(final_hidden_state)\n",
        "        logits=self.fc2(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvr72Cgg1z_B"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = RNNClassifier(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9kPsNgT11CM"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w61SwJv2loC",
        "outputId": "e8cec123-423a-4a97-85f4-708558ded96d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "Train loss 1.0498058519113376 Train acc 0.41420880149812733\n",
            "Val loss 1.0540975860186985 Val acc 0.40599455040871935\n",
            "Test loss 1.034387070792062 Test acc 0.4122171945701357\n",
            "Epoch 1\n",
            "Train loss 1.0465956276722168 Train acc 0.4229868913857678\n",
            "Val loss 1.0387198311941965 Val acc 0.4396003633060854\n",
            "Test loss 1.0360353725297111 Test acc 0.43619909502262444\n",
            "Epoch 2\n",
            "Train loss 1.0458778267942563 Train acc 0.4225187265917603\n",
            "Val loss 1.0383976476533072 Val acc 0.41326067211625794\n",
            "Test loss 1.037851448569979 Test acc 0.42217194570135747\n",
            "Epoch 3\n",
            "Train loss 1.0464190266998519 Train acc 0.42310393258426965\n",
            "Val loss 1.0575488005365645 Val acc 0.40236148955495005\n",
            "Test loss 1.0362409106322696 Test acc 0.41312217194570133\n",
            "Epoch 4\n",
            "Train loss 1.045999890186367 Train acc 0.4233380149812734\n",
            "Val loss 1.0635463782719203 Val acc 0.40236148955495005\n",
            "Test loss 1.037119379213878 Test acc 0.4113122171945701\n",
            "Epoch 5\n",
            "Train loss 1.0452343886264701 Train acc 0.4243913857677903\n",
            "Val loss 1.0559881619044713 Val acc 0.4014532243415077\n",
            "Test loss 1.0375275926930563 Test acc 0.4122171945701357\n",
            "Epoch 6\n",
            "Train loss 1.045327916350704 Train acc 0.42404026217228463\n",
            "Val loss 1.0674907105309623 Val acc 0.39509536784741145\n",
            "Test loss 1.040950686591012 Test acc 0.4095022624434389\n",
            "Epoch 7\n",
            "Train loss 1.0448678381434093 Train acc 0.42368913857677903\n",
            "Val loss 1.0621789847101484 Val acc 0.39691189827429607\n",
            "Test loss 1.042990802867072 Test acc 0.4104072398190045\n",
            "Epoch 8\n",
            "Train loss 1.0445084732570005 Train acc 0.4234550561797753\n",
            "Val loss 1.0599869455610003 Val acc 0.3978201634877384\n",
            "Test loss 1.041205200127193 Test acc 0.41266968325791853\n",
            "Epoch 9\n",
            "Train loss 1.0444502928730255 Train acc 0.4238061797752809\n",
            "Val loss 1.065470220361437 Val acc 0.40054495912806537\n",
            "Test loss 1.0449032723903655 Test acc 0.4117647058823529\n",
            "Epoch 10\n",
            "Train loss 1.0440681960698817 Train acc 0.42544475655430714\n",
            "Val loss 1.0621206521987916 Val acc 0.4223433242506812\n",
            "Test loss 1.040356775692531 Test acc 0.4239819004524887\n",
            "Epoch 11\n",
            "Train loss 1.0438690647650302 Train acc 0.4247425093632959\n",
            "Val loss 1.0566295402390615 Val acc 0.4296094459582198\n",
            "Test loss 1.0411453238555364 Test acc 0.4321266968325792\n",
            "Epoch 12\n",
            "Train loss 1.0441009984927232 Train acc 0.42404026217228463\n",
            "Val loss 1.0561980281557355 Val acc 0.43142597638510444\n",
            "Test loss 1.041502570254462 Test acc 0.4334841628959276\n",
            "Epoch 13\n",
            "Train loss 1.0435352586628346 Train acc 0.42532771535580527\n",
            "Val loss 1.058531495503017 Val acc 0.4296094459582198\n",
            "Test loss 1.0427428466933115 Test acc 0.4316742081447964\n",
            "Epoch 14\n",
            "Train loss 1.0429396274384488 Train acc 0.42579588014981273\n",
            "Val loss 1.052224566255297 Val acc 0.46321525885558584\n",
            "Test loss 1.0407088841710772 Test acc 0.4475113122171946\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 155\n",
        "train_acc_arr=[]\n",
        "val_acc_arr=[]\n",
        "train_lss_arr=[]\n",
        "test_lss_arr=[]\n",
        "test_acc_arr=[]\n",
        "val_lss_arr=[]\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iter, optimizer, criterion)  # Pass device\n",
        "    val_loss, val_acc = evaluate(model, val_iter, criterion)\n",
        "    test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
        "    test_lss_arr.append(test_loss)\n",
        "    test_acc_arr.append(test_acc)\n",
        "    train_acc_arr.append(train_acc)\n",
        "    val_acc_arr.append(val_acc)\n",
        "    train_lss_arr.append(train_loss)\n",
        "    val_lss_arr.append(val_loss)\n",
        "    print(\"Epoch\",epoch)\n",
        "    print(\"Train loss\",train_loss,\"Train acc\",train_acc)\n",
        "    print(\"Val loss\",val_loss,\"Val acc\",val_acc)\n",
        "    print(\"Test loss\",test_loss,\"Test acc\",test_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDTcMLMK2n7e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x_xis=np.arange(0,50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_dAYaLJqVxpI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0irxbNlMpA9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRsVnuJ6OTf-"
      },
      "source": [
        "Training with different model for higher degree of accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LTPoyCpOcy0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}